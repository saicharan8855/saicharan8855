## Saicharan K

**Aspiring Applied Scientist** · BSc Data Science (2024–2027) · Building ML from first principles, not imports.

I study the mathematics underneath machine learning — deriving cost functions, proving convergence, implementing algorithms from scratch. My approach: **First Principles → Mathematical Proof → Implementation**. If I can't derive it, I don't claim to know it.

---

### Featured Project

#### [Linear Regression from Scratch](https://github.com/saicharan8855/linear-regression-from-scratch)

Full mathematical derivation and implementation of linear regression — gradient descent, normal equation, and L2 regularization. **No sklearn. No abstractions.** Every line traces back to the math.

**Regularized cost function:**

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$$

**Gradient update rule:**

$$\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]$$

**Normal equation (closed-form solution):**

$$\theta = \left( X^T X + \lambda I \right)^{-1} X^T y$$

> The repository includes the full derivation of why MSE is the right loss under Gaussian noise assumptions, and why gradient descent converges for convex objectives.

---

### Currently Learning / Building

- **Logistic regression from scratch** — deriving the cross-entropy loss from maximum likelihood estimation
- **Bias-variance decomposition** — formal proof and empirical demonstration on polynomial regression
- **Probability theory** — measure-theoretic foundations, convergence of random variables

---

### Writing

| Article | Link |
|:---|:---|
| **Why Minimizing MSE is Not a Guess** — A derivation showing MSE emerges naturally from maximum likelihood under Gaussian assumptions | [Read on Hashnode](https://saicharan88.hashnode.dev/why-minimizing-mse-is-not-a-guess) |

More at [hashnode.com/@Saicharan88](https://hashnode.com/@Saicharan88)

---

### Certifications

| Course | Platform | Verification |
|:---|:---|:---|
| Machine Learning — Andrew Ng | Coursera | [Verified](https://www.coursera.org/account/accomplishments/verify/1H1FC410K0R4) |
| Python Bootcamp: Zero to Hero | Udemy | Completed |
| SQL (Basic) | HackerRank | [Verified](https://www.hackerrank.com/certificates/iframe/77f512d5ede8) |
| SQL (Intermediate) | HackerRank | [Verified](https://www.hackerrank.com/certificates/iframe/45aaefbbbaf3) |

---

### Tools

`Python` · `NumPy` · `Pandas` · `Matplotlib` · `Seaborn` · `Scikit-learn` · `TensorFlow` · `PyTorch` · `PostgreSQL` · `Jupyter` · `Git` · `Linux` · `Kaggle` 

---

### GitHub Stats

<div align="center">

<img width="48%" src="https://github-readme-streak-stats.herokuapp.com/?user=saicharan8855&background=0d1117&border=30363d&stroke=30363d&ring=58a6ff&fire=58a6ff&currStreakNum=c9d1d9&sideNums=c9d1d9&currStreakLabel=58a6ff&sideLabels=8b949e&dates=8b949e" alt="GitHub Streak"/>
&nbsp;
<img width="48%" src="https://github-readme-stats.vercel.app/api/top-langs/?username=saicharan8855&layout=compact&bg_color=0d1117&border_color=30363d&title_color=58a6ff&text_color=c9d1d9&hide_border=false" alt="Top Languages"/>

</div>

---

<div align="center">

<a href="https://www.linkedin.com/in/saicharan-k-a7b5a5267/">LinkedIn</a> · <a href="https://www.kaggle.com/saicharan8855">Kaggle</a> · <a href="https://hashnode.com/@Saicharan88">Hashnode</a> · <a href="https://github.com/saicharan8855">GitHub</a> · <a href="mailto:saicharan9948644390@gmail.com">saicharan9948644390@gmail.com</a>

</div>

<img width="100%" src="https://capsule-render.vercel.app/api?type=rect&color=0d1117&height=2&section=footer" />